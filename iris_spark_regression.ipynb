{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7d345f",
   "metadata": {},
   "source": [
    "#### regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7191c69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/07 21:16:24 WARN Instrumentation: [2071894b] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/10/07 21:16:24 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/10/07 21:16:24 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "25/10/07 21:16:26 WARN Instrumentation: [2fd36b53] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/10/07 21:16:27 WARN Instrumentation: [80610d62] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/10/07 21:16:28 WARN Instrumentation: [f72c9d9a] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/10/07 21:16:29 WARN Instrumentation: [20564c99] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集 RMSE: 0.7249646889209516\n",
      "测试集 RMSE: 0.7232588285989255\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"CA_Housing_builtin\").getOrCreate()\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "cal = fetch_california_housing()   \n",
    "pdf = pd.DataFrame(cal.data, columns=cal.feature_names)\n",
    "pdf['medianHouseValue'] = cal.target    \n",
    "\n",
    "housing = spark.createDataFrame(pdf)\n",
    "\n",
    "housing = housing.na.drop(how=\"any\")\n",
    "\n",
    "from pyspark.ml.feature import RFormula\n",
    "rf = RFormula(formula=\"medianHouseValue ~ .\")\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression()\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[rf, lr])\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\") \n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "params = ParamGridBuilder().addGrid(lr.regParam, [0, 0.1, 0.2]).build()\n",
    "cv = CrossValidator(estimator=pipeline,\n",
    "                    estimatorParamMaps=params,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=4)\n",
    "\n",
    "train, test = housing.randomSplit([0.7, 0.3])\n",
    "model = cv.fit(train)\n",
    "\n",
    "print(\"训练集 RMSE:\", evaluator.evaluate(model.transform(train)))\n",
    "print(\"测试集 RMSE:\", evaluator.evaluate(model.transform(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17fc4157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "epsilon: The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber (default: 1.35)\n",
      "featuresCol: features column name. (default: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label)\n",
      "loss: The loss function to be optimized. Supported options: squaredError, huber. (default: squaredError)\n",
      "maxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 100)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.0)\n",
      "solver: The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (default: auto)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(model.bestModel.stages[1].explainParams())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
